{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "team 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2q7VE4jQrmC"
      },
      "source": [
        "DSSP16 - polytechnique Data Challenge 11-12 December 2020\n",
        "\n",
        "Big Graphs & Text Mining with Spark in Python\n",
        "\n",
        "\n",
        "*Team 3 : Giulio Gallegos, Gueorgui Nedev, Christopher Cala, Salomon Hazan*\n",
        "\n",
        "# Table of contents <a id=\"Table of contents\">\n",
        "\n",
        "*   [Abstract](#Abstract)\n",
        "*   [Introduction](#Introduction)\n",
        "*   [Related work](#Related_work)\n",
        "*   [Data set](#Data_set) \n",
        "\n",
        "*   [Methodology](#Methodology)\\\n",
        "      -[Data Preparation](#Data_preparation)\\\n",
        "      -[Feature selection](#Feature_selection)\\\n",
        "      -[Graph features](#Graph_features)\\\n",
        "      -[Models](#Models)\n",
        "\n",
        "*   [Experiments](#Experiments)\\\n",
        "    -[Experimental setup](#Experimental_setup)\\\n",
        "    -[Baselines](#Baselines)\\\n",
        "    -[Evaluation](#Evaluation)\n",
        "\n",
        "*   [Results & discussion](#Results_&_discussion)\\\n",
        "    -[Sample issue](#Sample_issue)\n",
        "\n",
        "*   [Conclusion](#Conclusion)\n",
        "*   [Acknowledgements](#Acknowledgements)\n",
        "*   [References](#References)\n",
        "*   [Pyspark code](#Pyspark_code)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrFQN7ebBHey"
      },
      "source": [
        "# Abstract <a id=\"Abstract\">\n",
        "Wikipedia pages are linked with origin page text titles, links to destination page titles from other pages, introductory paragraph text for each page, and more features. Directional relationships between pages can be represented in a graph form. Text mining and graph analysis of such deep data sets of unstructured text and graph data including directions and edges can be done on a Spark cluster using Python and Structured Query Language commands to preprocess, clean, analyse, join, and extract meaningful features from the data to train and test supervised machine learning classification models to predict page link presence.\n",
        "From our two data sets of web page titles, introduction texts, graph of web page origin-destination page title links and a boolean classification of page link presence, we were able to reach over 99% performance as measured in Area Under the Precision Recall Curve with only seven features (three from the text data and four from the graph) and trials with three supervised classification models (ranging from Logistic Regression to Support Vector Classifier to Random Forest Classifier) using ten-fold cross validation with low risk of overfitting.\n",
        "Performance may be enhanced through further experimentation with other graph methods including StronglyConnectedComponents, Communities via LabelPropagation, PageRank & Graph of Words. Stopword list adjustment to Wikipedia type language corpus rather than the one we used from the reference film review language corpus example could also potentially improve results.\n",
        "Training and testing the model on new wikipedia datasets and generating more False boolean links could provide further insight for model optimization and utilization.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VWY91AqBMQB"
      },
      "source": [
        "# Introduction <a id=\"Introduction\">\n",
        "Starting with unstructured text data, titles and introductory paragraphs of wikipedia pages and semi-structured graph data titles of origin pages, destination pages, and a boolean indicating the presence of a link between the two titles, we aim to process, clean, analyse and extract relevant features from the data and predict links between wikipedia pages by leveraging a cluster of machines running on Apache Spark to parallelize the work. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9npD1AtMBSzW"
      },
      "source": [
        "# Related work <a id=\"Related_work\">\n",
        "Seminal works including Porter 1980 suffix stripping stemming algorithm, Page & Brin 1998 Page rank, Newman 2002 community structure, Manning 2008 introduction to information retrieval, Leskovec & Kleinberg from 2010 Kronecker graphs, and Giatsidis & Vazirgiannis from 2011 graph degeneracy, D-cores, and graph of words, provide the methods to convert complex text into simplified roots for further analysis including term frequencies, probabilistic information retrieval, vector space classification, machine learning, and link analysis with graphs.     \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJJq08xcBXtV"
      },
      "source": [
        "# Data set <a id=\"Data_set\">\n",
        "The unstructured text data \"Articles\" is in the form of a csv file containing two fields: Title of the Wikipedia page and Intro paragraph of the page.\n",
        "The semi-structured graph data 'Links' is in the form of a csv file representing he links between pages with three fields: Source (title of the origin page link), Target (title of the destination page link) and True/False (binary 1/0 indicating whether or not a true link exists) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZd25S8xiPEH"
      },
      "source": [
        "# Methodology <a id=\"Methodology\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5HItU-UBnEO"
      },
      "source": [
        "## Data preparation <a id=\"Data_preparation\">\n",
        "\n",
        "The unstructured text data underwent preprocessing, including removal of punctuation marks, conversion to lower case text, removal of common stop words, and stemming to bring words down to their roots (cf. Pyspark code lines 189-209, 212-256).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFkQy6lWBr7n"
      },
      "source": [
        "### Punctuation cleaning <a id=\"Punctuation_cleaning\">\n",
        "Punctuation was removed using regex as done in the text mining lab. We kept the parentheses as they are often used in Wikipedia titles to indicate disambiguation article titles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXr6fbKVBv52"
      },
      "source": [
        "### Lower case <a id=\"Lower_case\">\n",
        "The text case was harmonized to lower case in order faciliate further analyses.  For example, 'Everything' becomes 'everything' \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c-gsIb3B0Bj"
      },
      "source": [
        "### Stopwords removal <a id=\"Stopwords_removal\">\n",
        "Common stopwords appearing largely across the text corpus in a non-distinctive way were removed, using the English language stopwords list provided in the movie review lab. The list merits some second thought as many of its words reflect informally written language that appears in reviews, rather than more formal definition language that would appear in Wikipedia. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhS-rczpB7DQ"
      },
      "source": [
        "### Stemming <a id=\"Stemming\">\n",
        "The text data was further reduced to root words through stemming.  We used the Snowball stemmer in English to reduce the words to their word stems.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHlYlMTyB-7Q"
      },
      "source": [
        "## Feature selection  <a id=\"Feature_selection\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGdtpmhrCDJT"
      },
      "source": [
        "### Text features <a id=\"Text_features\">\n",
        "The source wikipedia page introduction text word count that was provided in example#2 was not sufficient in terms of feature importance, since it alone was equivalent to a random guess.  \n",
        "\n",
        "We added two types of additional features built from the text data. \\\n",
        " \\\n",
        " \n",
        "The first feature is the extent to which one finds the target wikipedia page title text (target.title) in the source wikipedia page introduction text (source.intro). This custom defined function ranges between 0 and 1 but is binary most of the time.  Because of this binary behavior, we think that evaluating it with anything more complex than logistic regression may be irrelevant for this feature by itself.  Evaluation with this first feature alone brings the performance up to 95%. (cf. Pyspark code lines 142-187)\\\n",
        " \\\n",
        "\n",
        "\n",
        "The second set of features derived from text data involved Term Frequency similarity.  We defined the Term Frequency as the sum over all words of the product of word occurences in the two sets of source and target Wikipedia page introductory texts, all divided by a power of the product of the total number of words in both documents. \\\n",
        "\n",
        " $$\\frac{\\sum_{w \\in Intro1 \\cup Intro2} count_1^w count_2^w}{(count_1 count_2)^\\alpha}$$   \n",
        " \\\n",
        "When $\\alpha=1$ it is biased toward short articles, and when $\\alpha<0.5$ it is biased toward long articles (cf. Pyspark code lines 212-254).  Evaluation with these two features alone, with $\\alpha=1$ and $\\alpha=0.5$ brought performance of 91%. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzRFdmyDCK8M"
      },
      "source": [
        "## Graph features <a id=\"Graph_features\">\n",
        "We transformed the semi-structured graph dataframe into a graph object by integrating the vertices and edges from the links dataframe into a graphframe using the GraphFrames library in order to extract in degrees and out degrees of each set of nodes, source wikipedia page title and destination page title, as well as to calculate other features such as the ratio degree, strongly connected components and communities.  \\\n",
        " \\\n",
        "We created structural components for further analysis by counting the number of double and triple links among the titles of source and destination Wikipedia page titles (cf. Pyspark code lines 28-60).  By mapping and reducing the links we were able to create two features, the number of double links and the number of triple links.    \n",
        "\n",
        "\n",
        "The addition of four graph features (degree ratio of source page, degree ratio of destination page, number of double links, and number of triple links) brought performance to over 99% without adding strongly connected components or communities (cf. Pyspark code lines 63-97).  We think integrating the raw in degrees and out degrees rather than the composite degree ratio may allow for better performance.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aGd__RBCOsn"
      },
      "source": [
        "## Models <a id=\"Models\">\n",
        "Three supervised learning classifier models were used to predict Wikipedia webpage links, Logistic Regression, Support Vector Classifier, and Random Forests.  The choice of these classifiers was motivated by the small number of features.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8D_8bDCS47"
      },
      "source": [
        "# Experiments <a id=\"Experiments\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHccBM1ICXCQ"
      },
      "source": [
        "## Experimental setup <a id=\"Experimental_setup\">\n",
        "Features were transformed into a sparse array and the logistic regression model was trained and tested using a ten-fold cross validataion set and hyperparameters were tuned using gridsearch. \n",
        "Logistic regression on one feature of wikipedia source page introduction paragraph word count was our baseline.  \n",
        "Support Vector Classifier was also trained and tested using a ten-fold cross validation set and hyperparameters were tuned using gridsearch.\n",
        "Finally, Random Forest Classifier was trained and tested using a ten-fold cross validataion set and hyperparameters were tuned using gridsearch  (cf. Pyspark code lines 265-373). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZFfL6iCanp"
      },
      "source": [
        "## Baselines  <a id=\"Baselines\">\n",
        "We consider the baseline to be the performance achieved with Logistic Regression Classifier on one Wikipedia page text introduction paragraph length feature as described in example2.  \n",
        "Model performance vs. baseline is outlined in the table below:\n",
        "\n",
        "**Table 1: Model performance vs baseline**\n",
        "\n",
        "| Feature set\\Model                                          |Logistic Regression Classifier | Support Vector Classifier | Random Forest Classifier\n",
        "|------------------------------------------------|-----------|---------|----|\n",
        "| example2.py feature     |    53.165%    |    53.181%    |    68.575%    |\n",
        "| Team 3 text features    |    96.949%    |    97.105%    |    97.163%    |\n",
        "| Team 3 graph features     |    97.477%    |    96.118%    |    99.999%    |\n",
        "| Team 3 text & graph features     |    99.921%    |    99.884%    |    99.999%    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC3NdDdkCeS7"
      },
      "source": [
        "## Evaluation <a id=\"Evaluation\">\n",
        "\n",
        "Each model was trained and tested using a ten-fold cross validation set and performance evaluated by Area under Precision Recall Curve. \\\n",
        "According to the results in table 1 above, the Random Forest Classifier performs best with the features we engineered at a performance over 99% vs a baseline of 53%. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNT116DkCiMM"
      },
      "source": [
        "# Results & discussion <a id=\"Results_&_discussion\">\n",
        "\n",
        "With a very limited set of features, starting with one, we were able to reach over 96% performance. By integrating both features derived from the text data and incrementing with features derived from graph data we were able to extend to near 100% performance, while still remaining under 10 features (seven total), but with a depth of samples.  \\\n",
        "\n",
        "Model selection and hyperparameter tuning also helped expand performance closer to 100% on a limited number of features (seven), yet with a depth of samples.   \n",
        " \n",
        "\n",
        "Our challenges were inherent to having deep sample data distributed on a spark cluster.  Spark cluster submissions were very long and often broke, even with sampling and it would have been perhaps easier to work with smaller sample sets on local machines for more efficient code prototyping and testing.  \n",
        "\n",
        "\n",
        "We had difficulties with the spark SQL joins at first and came across some disparities in row counts post-join for one of the joins, which although small at under a percentage point of difference, may perhaps be responsible for not reaching 100% performance.  \n",
        "\n",
        "\n",
        "Another problem inherent to the Spark ecosystem is not having full ability to exploit full python packages & libraries.  As an example, GraphFrames on Spark offers a more limited scope of functions than GraphX that was used in the labs.  \n",
        "\n",
        "\n",
        "Further experimentation with other graph methods including StronglyConnectedComponents, Comunities via LabelPropagation, PageRank & Graph of Words could yield promising results pending further research into the best ways to deploy these on Spark clusters. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG6uGZr8DdNw"
      },
      "source": [
        "## Sample issue <a id=\"Sample_issue\">\n",
        "\n",
        "**To do** : we have to ask or to try a sample again regarding to rdd "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jweq8hSClpt"
      },
      "source": [
        "# Conclusion <a id=\"Conclusion\">\n",
        "The combination of deeply sampled unstructured text data of Wikipedia page titles & introductory paragraphs along with semi-structured graph data on Wikipedia page origin titles and destination titles and boolean presence of links allowed optimal use of a Spark cluster to perform text and graph data preprocessing, cleaning, joining, feature extraction and predictive modeling and ten-fold cross validation to reach excellent performance with under ten features and low risk of overfitting.   \n",
        "\n",
        "Our feature engineering yielded 7 features tested with three models using ten-fold cross validation to reach over 99% performance as measured in Area Under the Precision Recall Curve.\n",
        "\n",
        "\n",
        "We could expand performance through further experimentation with other graph methods including StronglyConnectedComponents, Communities via LabelPropagation, PageRank & Graph of Words.  Further optimization could be achieved by stopword list alignment with Wikipedia type language rather than the film review language example.\n",
        "\n",
        "\n",
        "Training and testing the model on new wikipedia datasets and generating more False boolean links could provide further insight for model optimization and utilization. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7wC0G31xnpR"
      },
      "source": [
        "# Final thoughts and further development <a id=\"Conclusion\">\n",
        "While it was not that obviuos from the very beginning, it so happened that the proposed DataLab problem easily allowed for virtually 100% precision predictions that first hint to ovefitting, although it is not the case. \n",
        "\n",
        "The reason for these near-perfect results are two-fold. \n",
        "\n",
        "First, the text features that we use, title of the target in the source intro, and then the comparison of term frequencies, would lead to 97% precision. One can suppose that better text-based features could lead to even better results. Given time constraints, we prefered to focus on the graph-based features, which greatly improved the precision. \n",
        "\n",
        "Second, it is the graph structure that hints to existing links. The proportion of fake links being about 50%, a random edge has a 50% chance of having a real link, which is many orders of magnitude higher than the probalility of a random Wikipedia page linking to another random Wikipedia page. We have thought about how the problem may change if the data were \"fake flooded\", i.e. for each couple (p1, p2), we generate an edge of fake link (that is, class = 0).  By introducing imbalanced classes, we may consider stratified k-fold cross-validation rather than standard 10-fold cross-validation to adjust for class imbalance. In this case, of course the graph structure becomes irrelevant, as it would be a complete graph. The methods to tackle such a problem could be A) Consider text features only (hence unrelated to the graph structure) that would probably lead to at least 97% precision. B) consider the graph structure of the existing (class 1) links in the *training* set only with a special model based on the text features only in case source or target is not in the training set. We think that such an aproach could achieve over 99% precision for this more difficult problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_As5G9Cq5x"
      },
      "source": [
        "# Acknowledgements <a id=\"Acknowledgements\">\n",
        "Special thanks to our teachers Christos Giatsidis, Apostolos N. Papadopoulos, Michalis Vazirgiannis, Johannes Lutzeyer, Changmin Wu, Christos Xypolopoulos, Ioannis Nikolentzos, Abdine Hadi,... who gave us the theoretical and practical pedagogical support we needed to accomplish this task.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI73YXZ2CuGp"
      },
      "source": [
        "# References <a id=\"References\">\n",
        "Rousseau, F. and M. Vazirgiannis (2013b). Graph-of-word and TW-IDF: New Approach to Ad Hoc IR. In Proceedings of the 22nd ACM CIKM ’13, pp. 59–68,\n",
        "Rousseau, F. and M. Vazirgiannis (2015a). Main Core Retention on Graph-of-words for Single-Document Keyword Extraction. In Proceedings of the 37th European Conference on Information Retrieval. ECIR ’15,\n",
        "Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008. http://www-nlp.stanford.edu/IR-book/\n",
        "“Indexing by Latent Semantic Analysis”, S.Deerwester, S.Dumais, T.Landauer, G.Fumas, R.Harshman, Journal of the Society for Information Science, 1990\n",
        "“Mining the Web: Discovering Knowledge from Hypertext Data”, Soumen Chakrabarti\n",
        "J. Leskovec. Modeling Large Social and Information Networks. Tutorial\n",
        "at ICML, 2009.\n",
        "J. McAuley. Data Mining and Predictive Analytics, UCSD, 2015.\n",
        "D. Easley and J. Kleinberg. Networks, Crowds, and Markets: Reasoning\n",
        "About a Highly Connected World. Cambridge University Press, 2010.\n",
        "J. Leskovec, D. Chakrabarti, J. Kleinberg, C. Faloutsos, Z. Ghahramani.\n",
        "Kronecker Graphs: An approach to modeling networks. JMLR, 2010.\n",
        "M.E.J. Newman and M. Girvan. Finding and evaluating community structure in\n",
        "networks. Physical Review E 69(02), 2004.\n",
        "M.E.J. Newman. Modularity and community structure in networks. PNAS, 103(23),\n",
        "2006.\n",
        "S.E. Schaeffer. Graph clustering. Computer Science Review 1(1), 2007.\n",
        "S. Fortunato. Community detection in graphs. Physics Reports 486 (3-5), 2010.\n",
        "M. Coscia, F. Giannotti, and D. Pedreschi. A classification for community discovery methods in complex networks. Statistical Analysis and Data Mining 4 (5), 2011.\n",
        "A. Arenas, J. Duch, A. Fernandez, and S. Gomez. Size reduction of complex networks preserving modularity. New J. Phys., 9(176), 2007.\n",
        "M. Girvan and M.E.J. Newman. Community structure in social and biological networks. PNAS 99(12), 2002.\n",
        "U. Brandes, D. Delling, M. Gaertler, R. Gorke, M. Hoefer, Z. Nikoloski, and D. Wagner. On Modularity Clustering. IEEE TKDE 20(2), 2008.\n",
        "M.E.J. Newman. Fast algorithm for detecting community structure in networks. Phys. Rev. E 69, 2004.\n",
        "A. Clauset, M.E.J. Newman, and C. Moore. Finding community structure in very large networks. Phys. Rev. E 70, 2004.\n",
        "C. Giatsidis, D. Thilikos, and M. Vazirgiannis, D-cores: Measuring Collaboration of Directed Graphs Based on Degeneracy. Knowledge and Information Systems Journal, Springer, 2012.\n",
        "C. Giatsidis, K. Berberich, D. M. Thilikos, M. Vazirgiannis, Visual exploration of collaboration networks based ongraph degeneracy, ACM KDD, 2012.\n",
        "C. Giatsidis, D. Thilikos, and M. Vazirgiannis, D-cores: Measuring Collaboration of Directed Graphs Based on Degeneracy. IEEE ICDM, 2011,\n",
        "C. Giatsidis, D. Thilikos, and M. Vazirgiannis, Evaluating Cooperation in Communities with the kCore Structure. ACM/IEEE ASONAM, 2011.\n",
        "F. D. Malliaros and M. Vazirgiannis, To Stay or Not to Stay: Modeling Engagement Dynamics in Social Graphs. ACM CIKM, 2013.\n",
        "F. D. Malliaros and M. Vazirgiannis, Clustering and Community Detection in Directed Networks: A Survey. Physics Reports, 533(4), Elsevier, 2013.\n",
        "F. D. Malliaros and M. Vazirgiannis, Vulnerability Assessment in Social Networks under Cascadebased Node Departures, EPL (Europhysics Letters), 11(6), 2015.\n",
        "C. Giatsidis, F.D. Malliaros, D. Thilikos, and M. Vazirgiannis, CORECLUSTER: A Degeneracy Based Graph Clustering Framework. AAAI, 2014.\n",
        "F.D. Malliaros, V. Megalooikonomou and C. Faloutsos. Estimating Robustness in Large Social Graphs. Knowledge and Information Systems (KAIS), Springer, 2015.\n",
        "M.-E. G. Rossi, F.D. Malliaros, and M. Vazirgiannis, Spread It Good, Spread It Fast: Identification of Influential Nodes in Social Networks. WWW, 2015.\n",
        "C. Giatsidis, F. D. Malliaros and M. Vazirgiannis, Graph Mining Tools for Community Detection and Evaluation in Social Networks and the Web. WWW, Rome, Italy, 2013.\n",
        "C. Giatsidis, F. D. Malliaros and M. Vazirgiannis, Advanced graph mining for\n",
        "community evaluation in social networks and the web, ACM WSDM, Rio de Janeiro,\n",
        "Brazil, 2013.\n",
        "C. Giatsidis, F. D. Malliaros and M. Vazirgiannis, Community Detection and Evaluation in Social and Information Networks. WISE, Thessaloniki, Greece, 2014.\n",
        "F. D. Malliaros, M. Vazirgiannis and A.N. Papadopoulos, Core Decomposition:\n",
        "Algorithms and Applications, IEEE/ACM ASONAM, Paris, France, 2015.\n",
        "F. D. Malliaros, A.N. Papadopoulos, Core Decomposition in Graphs: Concepts,\n",
        "Algorithms and Applications. ICDM, Barcelona, 2016. \n",
        "D. Billsus and M. J. Pazzani, “Learning collaborative information\n",
        "filters”, In Proceedings of the Fifteenth International Conference on\n",
        "Machine Learning, pages 46-54, 1998\n",
        "“A Comparative Study of Collaborative Filtering Algorithms”, Joonseok Lee, Mingxuan Sun, Guy Lebanon, http://arxiv.org/pdf/1205.3193.pdf\n",
        "A. Paterek. Improving regularized singular value decomposition for\n",
        "collaborative filtering, Statistics, 2007:2{5, 2007.\n",
        "J. Leskovec, A. Rajaraman and J. Ullman. Mining of Massive Datasets. Cambridge University Press, 2014.\n",
        "J. Breese, D. Heckerman and C. Kadie. Empirical Analysis of\n",
        "Predictive Algorithms for Collaborative Filtering. In Proceedings of\n",
        "the 14th Conference on Uncertainty in Artificial Intelligence, 1998.\n",
        "D. Billsus and M. J. Pazzani, “Learning collaborative information\n",
        "filters”, In Proceedings of the Fifteenth International Conference on\n",
        "Machine Learning, pages 46{54, 1998\n",
        "“A Comparative Study of Collaborative Filtering Algorithms”, Joonseok Lee, Mingxuan Sun, Guy Lebanon, http://arxiv.org/pdf/1205.3193.pdf\n",
        "A. Paterek. Improving regularized singular value decomposition for\n",
        "collaborative filtering, Statistics, 2007:2{5, 2007.\n",
        "J. Leskovec, A. Rajaraman and J. Ullman. Mining of Massive\n",
        "Datasets. Cambridge University Press, 2014.\n",
        "J. Breese, D. Heckerman and C. Kadie. Empirical Analysis of\n",
        "Predictive Algorithms for Collaborative Filtering. In Proceedings of\n",
        "the 14th Conference on Uncertainty in Artificial Intelligence, 1998.\n",
        "Page, Lawrence & Brin, Sergey & Motwani, Rajeev & Winograd, Terry. (1998). The PageRank Citation Ranking: Bringing Order to the Web. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CKABu7lK4qC"
      },
      "source": [
        "# Pyspark code <a id=\"Pyspark_code\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcxZHmeOGhg5"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import lower, col\n",
        "import numpy as np\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import ArrayType\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from operator import add\n",
        "from pyspark.sql.functions import struct\n",
        "import pyspark.sql.functions as sqlf\n",
        "\n",
        "spark = SparkSession.builder.appName('example').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "df_articles = spark.read.csv(\"/dssp/datacamp/articles_text.csv\",sep=\",\").toDF(\"Title\",\"Intro\")\n",
        "df_links = spark.read.csv(\"/dssp/datacamp/edges_class.csv\",sep=\",\").toDF(\"source\",\"target\",\"class\")\n",
        "df_articles = df_articles.withColumn(\"title_lower\",lower(col(\"Title\")))\n",
        "df_articles.createOrReplaceTempView(\"text\")\n",
        "df_links.createOrReplaceTempView(\"links\")\n",
        "print(\"nodes\")\n",
        "print(df_articles.count())\n",
        "print(\"edges\")\n",
        "print(df_links.count())\n",
        "\n",
        "\n",
        "df_double_links = spark.sql(\"\"\"Select links.source as source, links2.target as target \n",
        "                                from links \n",
        "                                left join links as links2 on links.target==links2.source\"\"\")\n",
        "\n",
        "\n",
        "print(\"length 2 paths\")\n",
        "print(df_double_links.count())\n",
        "df_double_links.createOrReplaceTempView(\"double_links\")\n",
        "\n",
        "df_triple_links = spark.sql(\"\"\"Select links.source as source, double_links.target as target \n",
        "                                from links \n",
        "                                left join double_links on links.target==double_links.source\"\"\")\n",
        "\n",
        "print(\"length 3 paths\")\n",
        "print(df_triple_links.count())\n",
        "\n",
        "\n",
        "df_double_links = df_double_links.rdd.map(lambda (x,y):((x,y),1.0)).reduceByKey(add).map(lambda ((x,y),n) : (x,y,n)).toDF()\n",
        "#print(df_double_links.count())\n",
        "\n",
        "df_double_links = df_double_links.select(col('_1').alias('source'), col('_2').alias('target'), col('_3').alias('numof2paths'))\n",
        "\n",
        "df_triple_links = df_triple_links.rdd.map(lambda (x,y):((x,y),1.0)).reduceByKey(add).map(lambda ((x,y),n) : (x,y,n)).toDF()\n",
        "#print(df_triple_links.count())\n",
        "\n",
        "df_triple_links = df_triple_links.select(col('_1').alias('source'), col('_2').alias('target'), col('_3').alias('numof3paths'))\n",
        "\n",
        "print(\"reduced length 2 paths\")\n",
        "print(df_double_links.count())\n",
        "print(\"reduced length 3 paths\")\n",
        "print(df_triple_links.count())\n",
        "df_double_links.createOrReplaceTempView(\"double_links\")\n",
        "df_triple_links.createOrReplaceTempView(\"triple_links\")\n",
        "\n",
        "\n",
        "# Graph features \n",
        "from graphframes import *\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "#to build a graph we need vertices and edges\n",
        "v=df_links.rdd.flatMap(lambda x:[[x['source']],[x['target']]]).toDF([\"id\"])\n",
        "e=df_links.select(col(\"source\").alias(\"src\"),col(\"target\").alias(\"dst\"))\n",
        "\n",
        "g = GraphFrame(v, e)\n",
        "print(\"edges part of graph\")\n",
        "#print(g.edges.head(5))\n",
        "print(\"################\")\n",
        "df_indeg=g.inDegrees\n",
        "print(\"in degrees nodes\")\n",
        "print(df_indeg.count())\n",
        "print(\"################\")\n",
        "df_outdeg=g.outDegrees\n",
        "print(\"out degrees nodes\")\n",
        "print(df_outdeg.count())\n",
        "print(\"################\")\n",
        "#calculate degree ratio \n",
        "degreeRatio = df_indeg.join(df_outdeg, \"id\")\\\n",
        "  .selectExpr(\"id\", \"double(inDegree)/(double(outDegree)+1) as degreeRatio\")\n",
        "print(\"degreeRATIO nodes\")\n",
        "print(degreeRatio.count())\n",
        "print(\"################\")\n",
        "degreeRatio.createOrReplaceTempView(\"degreeRatio\")\n",
        "\n",
        "\n",
        "#scc = g.stronglyConnectedComponents(maxIter=3)\n",
        "#print(\"strongly connected components\")\n",
        "#print(scc.head(10))\n",
        "#communities = g.labelPropagation(maxIter=5)\n",
        "#print(\"communities\")\n",
        "#print(communities.head(10))\n",
        "\n",
        "inner_join = spark.sql(\"\"\"Select links.source as source,links.target as target,\n",
        "                        COALESCE(double_links.numof2paths,0) as L2_path, COALESCE(triple_links.numof3paths,0) as L3_path,\n",
        "                        COALESCE(degreeRatio.degreeRatio,0) as degratio_source, \n",
        "                        COALESCE(degreeRatio2.degreeRatio,0) as degratio_target, \n",
        "                        text1.Intro as txt1,text2.Intro as txt2,links.class as class \n",
        "                        from links\n",
        "                        left join double_links on (double_links.source==links.source and double_links.target==links.target)\n",
        "                        left join triple_links on (triple_links.source==links.source and triple_links.target==links.target)\n",
        "                        inner join text as text1 on text1.title_lower==links.source\n",
        "                        inner join text as text2 on text2.title_lower==links.target\n",
        "                        left join degreeRatio on degreeRatio.id==links.source \n",
        "                        left join degreeRatio as degreeRatio2 on degreeRatio2.id==links.target\n",
        "                        \"\"\")\n",
        "floatkeys = [\"class\", \"L2_path\", \"L3_path\", \"degratio_source\", \"degratio_target\"]\n",
        "\n",
        "\n",
        "# inner_join = spark.sql(\"\"\"Select links.source as source,links.target as target,\n",
        "                        # COALESCE(double_links.numof2paths,0) as L2_path, COALESCE(triple_links.numof3paths,0) as L3_path, \n",
        "                        # text1.Intro as txt1,text2.Intro as txt2,links.class as class \n",
        "                        # from links\n",
        "                        # left join double_links on (double_links.source==links.source and double_links.target==links.target)\n",
        "                        # left join triple_links on (triple_links.source==links.source and triple_links.target==links.target)\n",
        "                        # inner join text as text1 on text1.title_lower==links.source\n",
        "                        # inner join text as text2 on text2.title_lower==links.target\n",
        "                        # \"\"\")\n",
        "# floatkeys = [\"class\", \"L2_path\", \"L3_path\"]\n",
        "\n",
        "inner_join.head(10)\n",
        "print(\"size of innerjoin (should be equal to nb edges)\")\n",
        "\n",
        "print(inner_join.count())\n",
        "\n",
        "inner_join_distinct = inner_join.dropDuplicates()\n",
        "println(\"Distinct count inner_join: \"+inner_join_distinct.count())\n",
        "inner_join_disti.show(false)\n",
        "\n",
        "#sampling\n",
        "#inner_join = inner_join.randomSplit([0.05, 0.95], seed=0)\n",
        "#inner_join = inner_join.sample(False,0.1,42)\n",
        "\n",
        "# first sprint on punctuation\n",
        "import re\n",
        "\n",
        "#sum of number of occurences of words in list1 times num_occ in list2, all divided by (numberwordA * numberwordsB)^alpha \n",
        "def tf_similarity(list1, list2, alpha):\n",
        "    tf = 0.0\n",
        "    l1 = float(len(list1))\n",
        "    l2 = float(len(list2))\n",
        "    if l1*l2 == 0:\n",
        "        return 0\n",
        "    for w1 in list1:\n",
        "        for w2 in list2:\n",
        "            if w1==w2:\n",
        "                tf+=1\n",
        "    if tf == 0:\n",
        "        return 0\n",
        "\n",
        "    tf = tf / ((l1 * l2)**alpha)\t\n",
        "    return tf\t\t\n",
        "    \n",
        "\n",
        "#counts the words in a before parentheses. Counts what part of a is in b \n",
        "def b_contains_a(a, b):\n",
        "    i=0\n",
        "    maxi = 0\n",
        "    if len(a)==0:\n",
        "        return 0.0\n",
        "    for w in a:\n",
        "        if w != '(':\n",
        "            i+=1\n",
        "        else:\n",
        "            break\n",
        "    lena = i\n",
        "    if lena == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    i=0\n",
        "    for w in b:\n",
        "        if i == len(a):\n",
        "            return 1.0\n",
        "        if w == a[i]: \n",
        "            i+=1\n",
        "            maxi = i+1    \n",
        "            if maxi == lena:\n",
        "                return 1.0\n",
        "        else:\n",
        "            i=0\n",
        "    #return 3.0\n",
        "    return float(maxi)/float(lena)\n",
        "    \n",
        "# clean punctuation & remove stopwords\n",
        "def clean_punctuation_str(init_string):\n",
        "    string = init_string\n",
        "    string = re.sub(r'\\(', ' \\( ' , string)\n",
        "    string = re.sub(r'\\)', ' \\) ', string)\n",
        "    string = re.sub(r'[^\\w\\s()]',' ',string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string\n",
        "\n",
        "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
        "             'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
        "             'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
        "             'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
        "             'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
        "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "             'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
        "             'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
        "             'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
        "             'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
        "             'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
        "             'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'br']\n",
        "\n",
        "\n",
        "# create features with word count on source\n",
        "def transform(row):\n",
        "    data = row.asDict()  # Rows are immutable; We convert them to dictionaries\n",
        "    for key, value in data.items():\n",
        "        if not key in floatkeys:\n",
        "            if data[key] == None: data[key] = ''  # We want o avoid any null issues\n",
        "            data[key] = clean_punctuation_str(data[key].encode('utf-8'))\n",
        "            # lowercase and remove stopwords here\n",
        "            data[key] = data[key].encode('utf-8').lower()\n",
        "            data[key] = [w for w in data[key].split() if w not in stopwords]\n",
        "            #stemming\n",
        "            stemmer = SnowballStemmer(\"english\")\n",
        "            data[key] = [stemmer.stem(w) for w in data[key]]\n",
        "        else:\n",
        "            if data[key] == None: data[key]=0.0\n",
        "            #if dtaa[key] == Null: data[key]=0.0\n",
        "    # create a new column with an array of number to be the features\n",
        "    text1 = set([x for x in data['txt1']])\n",
        "    alpha = 0.5\n",
        "    beta = 1.0\n",
        "    a_in_b = b_contains_a(data[\"target\"], data[\"txt1\"])\n",
        "    tf1 = tf_similarity(data['txt1'],data['txt2'], alpha)\n",
        "    tf2 =tf_similarity(data['txt1'],data['txt2'], beta)\n",
        "    #data[\"features\"] = [len(text1), b_contains_a(data[\"target\"], data[\"txt1\"]), tf_similarity(data[\"txt1\"], data[\"txt1\"], alpha), tf_similarity(data[\"txt1\"], data[\"txt2\"], beta)]  # four features\n",
        "    #data[\"features\"] = [len(text1), b_contains_a(data[\"target\"], data[\"txt1\"]), tfsim2(data[\"txt1\"], data[\"txt1\"]), tfsim2(data[\"txt1\"], data[\"txt2\"])]  # four features\n",
        "    #data[\"features\"] = [a_in_b, tf1,tf2,float(data[\"L2_path\"]), float(data[\"L3_path\"]), data[\"degratio_source\"], data[\"degratio_target\"]]  # 7 features\n",
        "    #data[\"features\"] = [a_in_b, tf1,tf2,float(data[\"L2_path\"]), float(data[\"L3_path\"])]  # 5 features\n",
        "    data[\"features\"] = [data[\"txt1\"]]\n",
        "    #data[\"features\"] = [float(data[\"L2_path\"])]  # one feature features\n",
        "    \n",
        "    # keep only features and class\n",
        "    ret = {}\n",
        "#    ret[\"txt1\"] = data[\"txt1\"]\n",
        "#    ret[\"txt2\"] = data[\"txt2\"]\n",
        "#    ret[\"source\"] = data[\"source\"]\n",
        "#    ret[\"target\"] = data[\"target\"]\n",
        "    \n",
        "    ret[\"features\"] = data[\"features\"]\n",
        "    ret[\"class\"] = float(data[\"class\"])\n",
        "    # convert the dictionary back to a Row\n",
        "    newRow = Row(*ret.keys())  # a. the Row object specification (column names)\n",
        "    newRow = newRow(*ret.values())  # b. the corresponding column values\n",
        "    return newRow\n",
        "\n",
        "data=inner_join.rdd.map(transform).toDF()\n",
        "print(\"New dataframe with map on RDD\")\n",
        "#print(data.head(20))\n",
        "data.show(10)\n",
        "\n",
        "#data.summary().show()\n",
        "\n",
        "print(\"################\")\n",
        "\n",
        "#models use sparse arrays\n",
        "\n",
        "#Lets apply a function directly on the dataframe\n",
        "#UDF\n",
        "#applying functions to a DATAFRAME requires the \"SQL\" logic of\n",
        "#User Defined Functions (UDF)\n",
        "#as an example: convert features to sparse array\n",
        "#1. define what data type your UDF returns VectorUDT : UDF SCHEMA\n",
        "custom_udf_schema = VectorUDT()\n",
        "#2. define function\n",
        "def to_sparse_(v):\n",
        "        import numpy as np\n",
        "        if isinstance(v, SparseVector):\n",
        "            return v\n",
        "        vs = np.array(v)\n",
        "        nonzero = np.nonzero(vs)[0]\n",
        "        return SparseVector(len(v), nonzero, vs[nonzero])\n",
        "#3. create a udf from that function and the schema\n",
        "to_sparse = udf(to_sparse_,custom_udf_schema)\n",
        "#4. apply UDF to DF and create new column\n",
        "\n",
        "data= data.withColumn('feats',to_sparse(data.features))\n",
        "\n",
        "print(\"new sparse array column\")\n",
        "#print(data.head(1))\n",
        "print(\"################\")\n",
        "\n",
        "\n",
        "#Build models\n",
        "# Logistic Regression classifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "#Create estimator as logistic regression classifier\n",
        "logistic=LogisticRegression(featuresCol=\"feats\",labelCol=\"class\",predictionCol='class_pred',rawPredictionCol=\"class_pred_raw\",maxIter=10)\n",
        "\n",
        "#Set hyperparameters \n",
        "paramGrid = ParamGridBuilder().addGrid(logistic.regParam, [0.1, 0.01]).build()\n",
        "\n",
        "#Create evaluation metric as Area under the Precision Recall Curve\n",
        "evaluator_auc = BinaryClassificationEvaluator(rawPredictionCol=\"class_pred_raw\",labelCol='class',metricName=\"areaUnderPR\",)\n",
        "\n",
        "#Define cross validation function with above parameters and #folds to 10\n",
        "crossval = CrossValidator(estimator=logistic,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator_auc,\n",
        "                          numFolds=3)\n",
        "#Fit crossvalidator to dataset\n",
        "cvModel = crossval.fit(data)\n",
        "\n",
        "#Average scores of multiple folds on the parameters evaluated\n",
        "avg_scores=cvModel.avgMetrics\n",
        "\n",
        "print(avg_scores)\n",
        "\n",
        "# Support Vector Classifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "#Create estimator as support vector classifier\n",
        "svm=LinearSVC(featuresCol=\"feats\",labelCol=\"class\",predictionCol='class_pred',rawPredictionCol=\"class_pred_raw\",maxIter=10)\n",
        "\n",
        "#Set hyperparameters \n",
        "paramGrid = ParamGridBuilder().addGrid(svm.regParam, [0.1, 0.01]).build()\n",
        "\n",
        "#Create evaluation metric as Area under the Precision Recall Curve\n",
        "evaluator_auc = BinaryClassificationEvaluator(rawPredictionCol=\"class_pred_raw\",labelCol='class',metricName=\"areaUnderPR\",)\n",
        "\n",
        "#Define cross validation function with above parameters and #folds to 10\n",
        "crossval = CrossValidator(estimator=svm,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator_auc,\n",
        "                          numFolds=3)\n",
        "#Fit crossvalidator to dataset\n",
        "cvModel = crossval.fit(data)\n",
        "\n",
        "#Average scores of the multiple folds on the parameters evaluated\n",
        "avg_scores=cvModel.avgMetrics\n",
        "\n",
        "print(avg_scores)\n",
        "\n",
        "#Random Forest\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "#Create estimator as random forest classifier\n",
        "rf = RandomForestClassifier(featuresCol = 'feats', labelCol = 'class',predictionCol='class_pred',rawPredictionCol=\"class_pred_raw\")\n",
        "\n",
        "#Set hyperparameters \n",
        "paramGrid = ParamGridBuilder().addGrid(RandomForestClassifier.maxDepth, [4, 6, 8]).build()\n",
        "\n",
        "#Create evaluation metric as Area under the Precision Recall Curve\n",
        "evaluator_auc = BinaryClassificationEvaluator(rawPredictionCol=\"class_pred_raw\",labelCol='class',metricName=\"areaUnderPR\",)\n",
        "\n",
        "#Define cross validation function with above parameters and #folds to 10\n",
        "crossval = CrossValidator(estimator=rf,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator_auc,\n",
        "                          numFolds=3)\n",
        "#Fit crossvalidator to dataset\n",
        "cvModel = crossval.fit(data)\n",
        "\n",
        "#Average scores of the multiple folds on the parameters evaluated\n",
        "avg_scores=cvModel.avgMetrics\n",
        "\n",
        "print(avg_scores)\n",
        "\n",
        "<\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av2bCz_necUu"
      },
      "source": [
        "**Document modifications :**\n",
        "\n",
        "*   ...\n",
        "*   19 dec. put global doc into ArXiv publication type structure with table of contents, introduction, methodology, results, discussion, conclusion, ...\n",
        "*   13 dec. **Salomon** :  Layout and adding points 3,4,5 to complete.\n",
        "*   12 dec. **Gueorgui** : I'm throwing here som preliminary notes on what we have done (and what we will have done) on the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIit8NuYehKe"
      },
      "source": [
        "19 dec\n",
        "Gueorgui Nedev is inviting you to a scheduled Zoom meeting.\n",
        "\n",
        "Topic: Gueorgui Nedev's Personal Meeting Room\n",
        "\n",
        "Join Zoom Meeting\n",
        "https://us04web.zoom.us/j/5962690709?pwd=yJ9rjDQ0YPI\n",
        "\n",
        "Meeting ID: 596 269 0709\n",
        "Passcode: 0xptj1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe_VNHguevcb"
      },
      "source": [
        "#To Use in jupiter to convert into PDF\n",
        "pip install -U notebook-as-pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRR7Bq24Mbm8"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import lower, col\n",
        "import numpy as np\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import ArrayType\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from operator import add\n",
        "from pyspark.sql.functions import struct\n",
        "import pyspark.sql.functions as sqlf\n",
        "\n",
        "spark = SparkSession.builder.appName('example').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "df_articles = spark.read.csv(\"/dssp/datacamp/articles_text.csv\",sep=\",\").toDF(\"Title\",\"Intro\")\n",
        "df_links = spark.read.csv(\"/dssp/datacamp/edges_class.csv\",sep=\",\").toDF(\"source\",\"target\",\"class\")\n",
        "df_articles = df_articles.withColumn(\"title_lower\",lower(col(\"Title\")))\n",
        "\n",
        "\n",
        "print(\"articles\")\n",
        "df_articles = df_articles.dropDuplicates(['title_lower'])\n",
        "print(df_articles.count())\n",
        "\n",
        "print(\"edges\")\n",
        "df_links = df_links.dropDuplicates(['source', 'target'])\n",
        "print(df_links.count())\n",
        "\n",
        "print(\"real edges\")\n",
        "class1_links = df_links.filter(\"class = 1\")\n",
        "print(class1_links.count())\n",
        "\n",
        "\n",
        "\n",
        "df_articles.createOrReplaceTempView(\"text\")\n",
        "df_links.createOrReplaceTempView(\"links\")\n",
        "\n",
        "(train,test)=df_links.rdd.randomSplit([0.8,0.2])\n",
        "\n",
        "train = train.toDF()\n",
        "class1_train = train.filter(\"class = 1\")\n",
        "test = test.toDF()\n",
        "#class1_train.createOrReplaceTempView(\"links\")\n",
        "train.createOrReplaceTempView(\"links\")\n",
        "\n",
        "df_double_links = spark.sql(\"\"\"Select links.source as source, links2.target as target \n",
        "                                from links \n",
        "                                left join links as links2 on links.target==links2.source\"\"\")\n",
        "\n",
        "# for each A to B in links and B to C in links we make line A to B to C and then keep only A to C\n",
        "\n",
        "\n",
        "print(\"length 2 paths\")\n",
        "print(df_double_links.count())\n",
        "df_double_links.createOrReplaceTempView(\"double_links\")\n",
        "\n",
        "df_triple_links = spark.sql(\"\"\"Select links.source as source, double_links.target as target \n",
        "                                from links \n",
        "                                left join double_links on links.target==double_links.source\"\"\")\n",
        "\n",
        "print(\"length 3 paths\")\n",
        "print(df_triple_links.count())\n",
        "\n",
        "df_triple_links.createOrReplaceTempView(\"triple_links\")\n",
        "\n",
        "df_quadruple_links = spark.sql(\"\"\"Select links.source as source, triple_links.target as target \n",
        "                                from links \n",
        "                                left join triple_links on links.target==triple_links.source\"\"\")\n",
        "\n",
        "print(\"length 4 paths\")\n",
        "print(df_quadruple_links.count())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_double_links = df_double_links.rdd.map(lambda (x,y):((x,y),1.0)).reduceByKey(add).map(lambda ((x,y),n) : (x,y,n)).toDF()\n",
        "#print(df_double_links.count())\n",
        "\n",
        "df_double_links = df_double_links.select(col('_1').alias('source'), col('_2').alias('target'), col('_3').alias('numof2paths'))\n",
        "\n",
        "df_triple_links = df_triple_links.rdd.map(lambda (x,y):((x,y),1.0)).reduceByKey(add).map(lambda ((x,y),n) : (x,y,n)).toDF()\n",
        "#print(df_triple_links.count())\n",
        "\n",
        "df_triple_links = df_triple_links.select(col('_1').alias('source'), col('_2').alias('target'), col('_3').alias('numof3paths'))\n",
        "\n",
        "\n",
        "df_quadruple_links = df_quadruple_links.rdd.map(lambda (x,y):((x,y),1.0)).reduceByKey(add).map(lambda ((x,y),n) : (x,y,n)).toDF()\n",
        "#print(df_triple_links.count())\n",
        "\n",
        "df_quadruple_links = df_quadruple_links.select(col('_1').alias('source'), col('_2').alias('target'), col('_3').alias('numof4paths'))\n",
        "\n",
        "\n",
        "\n",
        "print(\"reduced length 2 paths\")\n",
        "print(df_double_links.count())\n",
        "print(\"reduced length 3 paths\")\n",
        "print(df_triple_links.count())\n",
        "print(\"reduced length 4 paths\")\n",
        "print(df_quadruple_links.count())\n",
        "\n",
        "df_double_links.createOrReplaceTempView(\"double_links\")\n",
        "df_triple_links.createOrReplaceTempView(\"triple_links\")\n",
        "df_quadruple_links.createOrReplaceTempView(\"quadruple_links\")\n",
        "\n",
        "\n",
        "# Graph features \n",
        "from graphframes import *\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "#to build a graph we need vertices and edges\n",
        "v=df_links.rdd.flatMap(lambda x:[[x['source']],[x['target']]]).toDF([\"id\"])\n",
        "e=train.select(col(\"source\").alias(\"src\"),col(\"target\").alias(\"dst\"))\n",
        "\n",
        "g = GraphFrame(v, e)\n",
        "print(\"edges part of graph\")\n",
        "#print(g.edges.head(5))\n",
        "print(\"################\")\n",
        "df_indeg=g.inDegrees\n",
        "print(\"in degrees nodes\")\n",
        "print(df_indeg.count())\n",
        "print(\"################\")\n",
        "df_outdeg=g.outDegrees\n",
        "print(\"out degrees nodes\")\n",
        "print(df_outdeg.count())\n",
        "print(\"################\")\n",
        "#calculate degree ratio \n",
        "degreeRatio = df_indeg.join(df_outdeg, \"id\")\\\n",
        "  .selectExpr(\"id\", \"double(inDegree)/(double(outDegree)+1) as degreeRatio\")\n",
        "print(\"degreeRATIO nodes\")\n",
        "print(degreeRatio.count())\n",
        "print(\"################\")\n",
        "degreeRatio.createOrReplaceTempView(\"degreeRatio\")\n",
        "df_indeg.createOrReplaceTempView(\"indegree\")\n",
        "df_outdeg.createOrReplaceTempView(\"outdegree\")\n",
        "\n",
        "# degreejoin = spark.sql(\"\"\"Select indegree.id as id, COALESCE(indegree.inDegree,0) as indeg, COALESCE(outdegree.outDegree,0) as outdeg,\n",
        "#                         from indegree left join outdegree on indegree.id==outdegree.id\n",
        "#                         \"\"\")\n",
        "\n",
        "\n",
        "degreejoin = spark.sql(\"\"\"Select indegree.id as id, COALESCE(indegree.inDegree,0) as indeg, COALESCE(outdegree.outDegree,0) as outdeg\n",
        "                        from indegree full join outdegree on indegree.id==outdegree.id\n",
        "                        \"\"\")\n",
        "\n",
        "degreejoin.createOrReplaceTempView(\"inoutdegrees\")\n",
        "\n",
        "#scc = g.stronglyConnectedComponents(maxIter=3)\n",
        "#print(\"strongly connected components\")\n",
        "#print(scc.head(10))\n",
        "#communities = g.labelPropagation(maxIter=5)\n",
        "#print(\"communities\")\n",
        "#print(communities.head(10))\n",
        "\n",
        "# inner_join = spark.sql(\"\"\"Select links.source as source,links.target as target,\n",
        "#                         COALESCE(double_links.numof2paths,0) as L2_path, COALESCE(triple_links.numof3paths,0) as L3_path,\n",
        "#                         COALESCE(degreeRatio.degreeRatio,0) as degratio_source, \n",
        "#                         COALESCE(degreeRatio2.degreeRatio,0) as degratio_target, \n",
        "#                         text1.Intro as txt1,text2.Intro as txt2,links.class as class \n",
        "#                         from links\n",
        "#                         left join double_links on (double_links.source==links.source and double_links.target==links.target)\n",
        "#                         left join triple_links on (triple_links.source==links.source and triple_links.target==links.target)\n",
        "#                         inner join text as text1 on text1.title_lower==links.source\n",
        "#                         inner join text as text2 on text2.title_lower==links.target\n",
        "#                         left join degreeRatio on degreeRatio.id==links.source \n",
        "#                         left join degreeRatio as degreeRatio2 on degreeRatio2.id==links.target\n",
        "#                         \"\"\")\n",
        "# floatkeys = [\"class\", \"L2_path\", \"L3_path\", \"degratio_source\", \"degratio_target\"]\n",
        "\n",
        "\n",
        "# inner_join = spark.sql(\"\"\"Select links.source as source,links.target as target,\n",
        "                        # COALESCE(double_links.numof2paths,0) as L2_path, COALESCE(triple_links.numof3paths,0) as L3_path, \n",
        "                        # text1.Intro as txt1,text2.Intro as txt2,links.class as class \n",
        "                        # from links\n",
        "                        # left join double_links on (double_links.source==links.source and double_links.target==links.target)\n",
        "                        # left join triple_links on (triple_links.source==links.source and triple_links.target==links.target)\n",
        "                        # inner join text as text1 on text1.title_lower==links.source\n",
        "                        # inner join text as text2 on text2.title_lower==links.target\n",
        "                        # \"\"\")\n",
        "# floatkeys = [\"class\", \"L2_path\", \"L3_path\"]\n",
        "train.createOrReplaceTempView(\"links\")\n",
        "inner_join_train = spark.sql(\"\"\"Select links.source as source,links.target as target,\n",
        "                        COALESCE(double_links.numof2paths,0) as L2_path, COALESCE(triple_links.numof3paths,0) as L3_path,\n",
        "                        COALESCE(quadruple_links.numof4paths,0) as L4_path,\n",
        "                        COALESCE(inoutdegrees.indeg,0) as indeg_source, \n",
        "                        COALESCE(inoutdegrees.outdeg,0) as outdeg_source, \n",
        "                        COALESCE(inoutdegrees2.indeg,0) as indeg_target, \n",
        "                        COALESCE(inoutdegrees2.outdeg,0) as outdeg_target, \n",
        "                        text1.Intro as txt1,text2.Intro as txt2,links.class as class \n",
        "                        from links\n",
        "                        left join double_links on (double_links.source==links.source and double_links.target==links.target)\n",
        "                        left join triple_links on (triple_links.source==links.source and triple_links.target==links.target)\n",
        "                        left join quadruple_links on (quadruple_links.source==links.source and quadruple_links.target==links.target)\n",
        "                        inner join text as text1 on text1.title_lower==links.source\n",
        "                        inner join text as text2 on text2.title_lower==links.target\n",
        "                        left join inoutdegrees on inoutdegrees.id==links.source \n",
        "                        left join inoutdegrees as inoutdegrees2 on inoutdegrees2.id==links.target\n",
        "                        \"\"\")\n",
        "test.createOrReplaceTempView(\"links\")\n",
        "inner_join_test = spark.sql(\"\"\"Select links.source as source,links.target as target,\n",
        "                        COALESCE(double_links.numof2paths,0) as L2_path, COALESCE(triple_links.numof3paths,0) as L3_path,\n",
        "                        COALESCE(quadruple_links.numof4paths,0) as L4_path,\n",
        "                        COALESCE(inoutdegrees.indeg,0) as indeg_source, \n",
        "                        COALESCE(inoutdegrees.outdeg,0) as outdeg_source, \n",
        "                        COALESCE(inoutdegrees2.indeg,0) as indeg_target, \n",
        "                        COALESCE(inoutdegrees2.outdeg,0) as outdeg_target, \n",
        "                        text1.Intro as txt1,text2.Intro as txt2,links.class as class \n",
        "                        from links\n",
        "                        left join double_links on (double_links.source==links.source and double_links.target==links.target)\n",
        "                        left join triple_links on (triple_links.source==links.source and triple_links.target==links.target)\n",
        "                        left join quadruple_links on (quadruple_links.source==links.source and quadruple_links.target==links.target)\n",
        "                        inner join text as text1 on text1.title_lower==links.source\n",
        "                        inner join text as text2 on text2.title_lower==links.target\n",
        "                        left join inoutdegrees on inoutdegrees.id==links.source \n",
        "                        left join inoutdegrees as inoutdegrees2 on inoutdegrees2.id==links.target\n",
        "                        \"\"\")\n",
        "\n",
        "floatkeys = [\"class\", \"L2_path\", \"L3_path\", \"L4_path\", \"indeg_source\", \"indeg_target\", \"outdeg_source\", \"outdeg_target\"]\n",
        "\n",
        "\n",
        "inner_join_train.head(10)\n",
        "print(\"size of innerjoin (should be equal to nb edges)\")\n",
        "\n",
        "print(inner_join_train.count())\n",
        "\n",
        "# inner_join = inner_join.dropDuplicates([\"source\", \"target\"])\n",
        "# print(\"Distinct count inner_join: \")\n",
        "# print(inner_join.count())\n",
        "\n",
        "\n",
        "#sampling\n",
        "#inner_join = inner_join.randomSplit([0.05, 0.95], seed=0)\n",
        "#inner_join = inner_join.sample(False,0.1,42)\n",
        "\n",
        "# first sprint on punctuation\n",
        "import re\n",
        "\n",
        "#sum of number of occurences of words in list1 times num_occ in list2, all divided by (numberwordA * numberwordsB)^alpha \n",
        "def tf_similarity(list1, list2, alpha):\n",
        "    tf = 0.0\n",
        "    l1 = float(len(list1))\n",
        "    l2 = float(len(list2))\n",
        "    if l1*l2 == 0:\n",
        "        return 0\n",
        "    for w1 in list1:\n",
        "        for w2 in list2:\n",
        "            if w1==w2:\n",
        "                tf+=1\n",
        "    if tf == 0:\n",
        "        return 0\n",
        "\n",
        "    tf = tf / ((l1 * l2)**alpha)\t\n",
        "    return tf\t\t\n",
        "    \n",
        "\n",
        "#counts the words in a before parantheses. Counts what part of a is in b \n",
        "def b_contains_a(a, b):\n",
        "    i=0\n",
        "    maxi = 0\n",
        "    if len(a)==0:\n",
        "        return 0.0\n",
        "    for w in a:\n",
        "        if w != '(':\n",
        "            i+=1\n",
        "        else:\n",
        "            break\n",
        "    lena = i\n",
        "    if lena == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    i=0\n",
        "    for w in b:\n",
        "        if i == len(a):\n",
        "            return 1.0\n",
        "        if w == a[i]: \n",
        "            i+=1\n",
        "            maxi = i+1    \n",
        "            if maxi == lena:\n",
        "                return 1.0\n",
        "        else:\n",
        "            i=0\n",
        "    #return 3.0\n",
        "    return float(maxi)/float(lena)\n",
        "    \n",
        "# clean punctunation\n",
        "def clean_punctuation_str(init_string):\n",
        "    string = init_string\n",
        "    string = re.sub(r'\\(', ' \\( ' , string)\n",
        "    string = re.sub(r'\\)', ' \\) ', string)\n",
        "    string = re.sub(r'[^\\w\\s()]',' ',string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string\n",
        "\n",
        "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
        "             'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
        "             'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
        "             'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
        "             'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
        "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "             'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
        "             'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
        "             'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
        "             'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
        "             'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
        "             'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'br']\n",
        "\n",
        "\n",
        "# create features with word count on source\n",
        "def transform(row):\n",
        "    data = row.asDict()  # Rows are immutable; We convert them to dictionaries\n",
        "    for key, value in data.items():\n",
        "        if not key in floatkeys:\n",
        "            if data[key] == None: data[key] = ''  # We want o avoid any null issues\n",
        "            data[key] = clean_punctuation_str(data[key].encode('utf-8'))\n",
        "            # lowercase and remove stopwords here\n",
        "            data[key] = data[key].encode('utf-8').lower()\n",
        "            data[key] = [w for w in data[key].split() if w not in stopwords]\n",
        "            #stemming\n",
        "            stemmer = SnowballStemmer(\"english\")\n",
        "            data[key] = [stemmer.stem(w) for w in data[key]]\n",
        "        else:\n",
        "            if data[key] == None: data[key]=0.0\n",
        "            #if dtaa[key] == Null: data[key]=0.0\n",
        "    # create a new column with an array of number to be the features\n",
        "    text1 = set([x for x in data['txt1']])\n",
        "    alpha = 0.5\n",
        "    beta = 1.0\n",
        "    a_in_b = b_contains_a(data[\"target\"], data[\"txt1\"])\n",
        "    tf1 = tf_similarity(data['txt1'],data['txt2'], alpha)\n",
        "    tf2 =tf_similarity(data['txt1'],data['txt2'], beta)\n",
        "    #data[\"features\"] = [len(text1), b_contains_a(data[\"target\"], data[\"txt1\"]), tf_similarity(data[\"txt1\"], data[\"txt1\"], alpha), tf_similarity(data[\"txt1\"], data[\"txt2\"], beta)]  # four features\n",
        "    #data[\"features\"] = [len(text1), b_contains_a(data[\"target\"], data[\"txt1\"]), tfsim2(data[\"txt1\"], data[\"txt1\"]), tfsim2(data[\"txt1\"], data[\"txt2\"])]  # four features\n",
        "    #data[\"features\"] = [a_in_b, tf1,tf2,float(data[\"L2_path\"]), float(data[\"L3_path\"]), data[\"degratio_source\"], data[\"degratio_target\"]]  # 7 features\n",
        "    # data[\"features\"] = [a_in_b, tf1,tf2,float(data[\"L2_path\"]), float(data[\"L3_path\"])]  # 5 features\n",
        "    #data[\"features\"] = [float(data[\"L2_path\"]), float(data[\"L3_path\"])]  # 2 features\n",
        "    #data[\"features\"] = [data[\"degratio_source\"], data[\"degratio_target\"]]  # 2 features\n",
        "    #data[\"features\"] = [float(data[\"L2_path\"])]  # one feature features\n",
        "    data[\"features\"] = [a_in_b, tf1,tf2,float(data[\"L2_path\"]), float(data[\"L3_path\"]),float(data[\"L4_path\"]), float(data[\"indeg_source\"]),\n",
        "            float(data[\"outdeg_source\"]),float(data[\"indeg_target\"]),float(data[\"outdeg_target\"])]  # 9 features\n",
        "    # data[\"features\"] = [float(data[\"indeg_source\"]),\n",
        "    #        float(data[\"outdeg_source\"]),float(data[\"indeg_target\"]),float(data[\"outdeg_target\"])]  # 4 features\n",
        "    #data[\"features\"] = [a_in_b, tf1,tf2]  # text features only\n",
        "    # data[\"features\"] = [float(data[\"L2_path\"]), float(data[\"L3_path\"]), float(data[\"indeg_source\"]),\n",
        "    #        float(data[\"outdeg_source\"]),float(data[\"indeg_target\"]),float(data[\"outdeg_target\"])]  # graph features\n",
        "    # data[\"features\"] = [float(data[\"L2_path\"])]  # 1 graph feature\n",
        "    #data[\"features\"] = [a_in_b, tf1,tf2,float(data[\"L2_path\"]), float(data[\"L3_path\"])]  # 5 features\n",
        "\n",
        "    # keep only features and class\n",
        "    ret = {}\n",
        "#    ret[\"txt1\"] = data[\"txt1\"]\n",
        "#    ret[\"txt2\"] = data[\"txt2\"]\n",
        "#    ret[\"source\"] = data[\"source\"]\n",
        "#    ret[\"target\"] = data[\"target\"]\n",
        "    \n",
        "    ret[\"features\"] = data[\"features\"]\n",
        "    ret[\"class\"] = float(data[\"class\"])\n",
        "    # convert the dictionary back to a Row\n",
        "    newRow = Row(*ret.keys())  # a. the Row object specification (column names)\n",
        "    newRow = newRow(*ret.values())  # b. the corresponding column values\n",
        "    return newRow\n",
        "\n",
        "train=inner_join_train.rdd.map(transform).toDF()\n",
        "test = inner_join_test.rdd.map(transform).toDF()\n",
        "print(\"New dataframe with map on RDD\")\n",
        "#print(data.head(20))\n",
        "train.show(10)\n",
        "\n",
        "#data.summary().show()\n",
        "\n",
        "print(\"################\")\n",
        "\n",
        "#models use sparse arrays\n",
        "\n",
        "#Lets apply a function directly on the dataframe\n",
        "#UDF\n",
        "#applying functions to a DATAFRAME requires the \"SQL\" logic of\n",
        "#User Defined Functions (UDF)\n",
        "#as an example: convert features to sparse array\n",
        "#1. define what data type your UDF returns VectorUDT : UDF SCHEMA\n",
        "custom_udf_schema = VectorUDT()\n",
        "#2. define function\n",
        "def to_sparse_(v):\n",
        "        import numpy as np\n",
        "        if isinstance(v, SparseVector):\n",
        "            return v\n",
        "        vs = np.array(v)\n",
        "        nonzero = np.nonzero(vs)[0]\n",
        "        return SparseVector(len(v), nonzero, vs[nonzero])\n",
        "#3. create a udf from that function and the schema\n",
        "to_sparse = udf(to_sparse_,custom_udf_schema)\n",
        "#4. apply UDF to DF and create new column\n",
        "\n",
        "#data= data.withColumn('feats',to_sparse(data.features))\n",
        "train= train.withColumn('feats',to_sparse(train.features))\n",
        "test= test.withColumn('feats',to_sparse(test.features))\n",
        "\n",
        "print(\"new sparse array column\")\n",
        "#print(data.head(1))\n",
        "print(\"################\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Build a model on our data\n",
        "# Logistic Regression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "\n",
        "#(train,test)=data.rdd.randomSplit([0.8,0.2])\n",
        "\n",
        "\n",
        "\n",
        "# we only have 10 iterations for this example\n",
        "# pay attention to the input/output columns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "logistic=LogisticRegression(featuresCol=\"feats\",labelCol=\"class\",predictionCol='class_pred',rawPredictionCol=\"class_pred_raw\",maxIter=100)\n",
        "lrModel = logistic.fit(train)\n",
        "result=lrModel.transform(test)\n",
        "print(\"prediction output\")\n",
        "print(result.head(1))\n",
        "print(\"################\")\n",
        "\n",
        "\n",
        "#perform evaluation\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"class_pred_raw\",labelCol='class',metricName=\"areaUnderPR\",)\n",
        "print(\"evaluation\")\n",
        "print(evaluator.evaluate(result))\n",
        "print(\"################\")\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol = 'feats', labelCol = 'class',predictionCol='class_pred',rawPredictionCol=\"class_pred_raw\")\n",
        "\n",
        "rfModel = rf.fit(train)\n",
        "result=rfModel.transform(test)\n",
        "print(\"prediction output\")\n",
        "print(result.head(1))\n",
        "print(\"################\")\n",
        "\n",
        "\n",
        "#perform evaluation\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"class_pred_raw\",labelCol='class',metricName=\"areaUnderPR\",)\n",
        "print(\"evaluation\")\n",
        "print(evaluator.evaluate(result))\n",
        "print(\"################\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}